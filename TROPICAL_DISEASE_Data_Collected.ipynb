{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pY0jXpfdLyjE"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "=================================================================================\n",
        "TROPICAL DISEASE PREDICTION DATA COLLECTION SYSTEM\n",
        "Central Africa Focus: DRC, Congo, CAR, Cameroon\n",
        "=================================================================================\n",
        "\n",
        "Author: Research Team\n",
        "Date: 2025\n",
        "Platform: Google Colab\n",
        "Purpose: Automated data collection for ICICT 2026 paper\n",
        "\n",
        "Data Sources:\n",
        "1. Our World in Data (OWID) - Malaria statistics\n",
        "2. WHO Global Health Observatory (GHO) - Disease data via API\n",
        "3. NASA POWER - Climate data\n",
        "4. World Bank - Socioeconomic indicators\n",
        "\n",
        "Output: Master dataset ready for machine learning\n",
        "=================================================================================\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 1: INSTALLATION & IMPORTS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"SECTION 1: Installing Required Libraries\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q wbdata pandas numpy matplotlib seaborn requests openpyxl\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import time\n",
        "import os\n",
        "from datetime import datetime\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "\n",
        "print(\"âœ… All libraries imported successfully!\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 2: CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"SECTION 2: Configuration Setup\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Configuration dictionary\n",
        "CONFIG = {\n",
        "    'countries_iso': ['COD', 'COG', 'CAF', 'CMR'],  # DRC, Congo, CAR, Cameroon\n",
        "    'country_names': {\n",
        "        'COD': 'DRC',\n",
        "        'COG': 'Congo',\n",
        "        'CAF': 'CAR',\n",
        "        'CMR': 'Cameroon'\n",
        "    },\n",
        "    'start_year': 2010,\n",
        "    'end_year': 2024,\n",
        "    'output_dir': '/content/data',\n",
        "    'plots_dir': '/content/plots'\n",
        "}\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
        "os.makedirs(CONFIG['plots_dir'], exist_ok=True)\n",
        "\n",
        "# Cities for climate data (major cities in each country)\n",
        "CITIES = {\n",
        "    # DRC\n",
        "    'Kinshasa': {'lat': -4.3276, 'lon': 15.3136, 'country': 'DRC'},\n",
        "    'Lubumbashi': {'lat': -11.6667, 'lon': 27.4667, 'country': 'DRC'},\n",
        "    'Mbuji_Mayi': {'lat': -6.1500, 'lon': 23.6000, 'country': 'DRC'},\n",
        "    'Kisangani': {'lat': 0.5167, 'lon': 25.2000, 'country': 'DRC'},\n",
        "    'Goma': {'lat': -1.6792, 'lon': 29.2228, 'country': 'DRC'},\n",
        "\n",
        "    # Congo\n",
        "    'Brazzaville': {'lat': -4.2634, 'lon': 15.2429, 'country': 'Congo'},\n",
        "    'Pointe_Noire': {'lat': -4.7692, 'lon': 11.8636, 'country': 'Congo'},\n",
        "\n",
        "    # CAR\n",
        "    'Bangui': {'lat': 4.3947, 'lon': 18.5582, 'country': 'CAR'},\n",
        "\n",
        "    # Cameroon\n",
        "    'Yaounde': {'lat': 3.8480, 'lon': 11.5021, 'country': 'Cameroon'},\n",
        "    'Douala': {'lat': 4.0511, 'lon': 9.7679, 'country': 'Cameroon'}\n",
        "}\n",
        "\n",
        "print(\"âœ… Configuration complete!\")\n",
        "print(f\"   Countries: {list(CONFIG['country_names'].values())}\")\n",
        "print(f\"   Period: {CONFIG['start_year']} - {CONFIG['end_year']}\")\n",
        "print(f\"   Cities: {len(CITIES)} major cities\")\n",
        "print(f\"   Output directory: {CONFIG['output_dir']}\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 3: DATA COLLECTION FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"SECTION 3: Defining Data Collection Functions\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3.1 Our World in Data (OWID) - Malaria Data\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def download_owid_malaria_data():\n",
        "    \"\"\"\n",
        "    Download malaria data from Our World in Data\n",
        "    This is the most reliable source with WHO-aggregated data\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Malaria deaths and incidence data\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nðŸ“¥ Downloading malaria data from Our World in Data...\")\n",
        "\n",
        "    datasets = {}\n",
        "\n",
        "    # URLs for different OWID malaria datasets\n",
        "    urls = {\n",
        "        'deaths': 'https://raw.githubusercontent.com/owid/owid-datasets/master/datasets/Malaria%20deaths%20by%20age%20-%20IHME%2C%20Global%20Burden%20of%20Disease/Malaria%20deaths%20by%20age%20-%20IHME%2C%20Global%20Burden%20of%20Disease.csv',\n",
        "        'incidence': 'https://raw.githubusercontent.com/owid/owid-datasets/master/datasets/Malaria%20-%20World%20Health%20Organization%20(2023)/Malaria%20-%20World%20Health%20Organization%20(2023).csv'\n",
        "    }\n",
        "\n",
        "    # Country name mapping for OWID\n",
        "    country_mapping = {\n",
        "        'Democratic Republic of Congo': 'DRC',\n",
        "        'Congo': 'Congo',\n",
        "        'Central African Republic': 'CAR',\n",
        "        'Cameroon': 'Cameroon'\n",
        "    }\n",
        "\n",
        "    for dataset_name, url in urls.items():\n",
        "        try:\n",
        "            df = pd.read_csv(url)\n",
        "            print(f\"   âœ… Downloaded {dataset_name}: {len(df)} total records\")\n",
        "\n",
        "            # Filter for target countries\n",
        "            target_countries = list(country_mapping.keys())\n",
        "            mask = df['Entity'].isin(target_countries)\n",
        "            filtered_df = df[mask].copy()\n",
        "\n",
        "            # Filter for years\n",
        "            filtered_df = filtered_df[\n",
        "                (filtered_df['Year'] >= CONFIG['start_year']) &\n",
        "                (filtered_df['Year'] <= CONFIG['end_year'])\n",
        "            ]\n",
        "\n",
        "            # Standardize country names\n",
        "            filtered_df['Country'] = filtered_df['Entity'].map(country_mapping)\n",
        "\n",
        "            # Save\n",
        "            filename = f\"{CONFIG['output_dir']}/OWID_Malaria_{dataset_name.title()}.csv\"\n",
        "            filtered_df.to_csv(filename, index=False)\n",
        "\n",
        "            datasets[dataset_name] = filtered_df\n",
        "            print(f\"      â†’ Filtered: {len(filtered_df)} records for Central Africa\")\n",
        "            print(f\"      â†’ Saved: {filename}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ Error downloading {dataset_name}: {e}\")\n",
        "            datasets[dataset_name] = None\n",
        "\n",
        "    return datasets\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3.2 WHO GHO API - Malaria Indicators\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def download_who_gho_data():\n",
        "    \"\"\"\n",
        "    Download malaria data from WHO Global Health Observatory API\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Combined WHO malaria indicators\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nðŸ“¥ Downloading WHO GHO malaria data via API...\")\n",
        "\n",
        "    base_url = \"https://ghoapi.azureedge.net/api\"\n",
        "\n",
        "    # WHO malaria indicators\n",
        "    indicators = {\n",
        "        'MALARIA_EST_CASES': 'Estimated_Malaria_Cases',\n",
        "        'MALARIA_EST_DEATHS': 'Estimated_Malaria_Deaths',\n",
        "        'MALARIA_CONF_CASES': 'Confirmed_Malaria_Cases',\n",
        "        'MALARIA_INCD': 'Malaria_Incidence_per_1000',\n",
        "        'MALARIA_MORT': 'Malaria_Mortality_per_100000'\n",
        "    }\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    for indicator_code, indicator_name in indicators.items():\n",
        "        print(f\"\\n   Processing: {indicator_name}\")\n",
        "\n",
        "        for country_code in CONFIG['countries_iso']:\n",
        "            try:\n",
        "                url = f\"{base_url}/{indicator_code}\"\n",
        "                params = {\n",
        "                    '$filter': f\"SpatialDim eq '{country_code}'\",\n",
        "                    '$format': 'json'\n",
        "                }\n",
        "\n",
        "                response = requests.get(url, params=params, timeout=30)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "\n",
        "                    if 'value' in data and len(data['value']) > 0:\n",
        "                        for record in data['value']:\n",
        "                            year = record.get('TimeDim')\n",
        "                            if year and CONFIG['start_year'] <= int(year) <= CONFIG['end_year']:\n",
        "                                all_data.append({\n",
        "                                    'Country_Code': country_code,\n",
        "                                    'Country': CONFIG['country_names'][country_code],\n",
        "                                    'Year': int(year),\n",
        "                                    'Indicator': indicator_name,\n",
        "                                    'Value': record.get('NumericValue'),\n",
        "                                    'Low_Estimate': record.get('Low'),\n",
        "                                    'High_Estimate': record.get('High')\n",
        "                                })\n",
        "\n",
        "                        count = len([r for r in data['value']\n",
        "                                   if CONFIG['start_year'] <= int(r.get('TimeDim', 0)) <= CONFIG['end_year']])\n",
        "                        print(f\"      âœ… {CONFIG['country_names'][country_code]}: {count} records\")\n",
        "                    else:\n",
        "                        print(f\"      âš ï¸  {CONFIG['country_names'][country_code]}: No data available\")\n",
        "                else:\n",
        "                    print(f\"      âŒ {CONFIG['country_names'][country_code]}: HTTP {response.status_code}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"      âŒ {CONFIG['country_names'][country_code]}: Error - {e}\")\n",
        "\n",
        "            time.sleep(0.5)  # Rate limiting\n",
        "\n",
        "    if all_data:\n",
        "        df = pd.DataFrame(all_data)\n",
        "\n",
        "        # Create pivot table for easier use\n",
        "        pivot_df = df.pivot_table(\n",
        "            index=['Country', 'Country_Code', 'Year'],\n",
        "            columns='Indicator',\n",
        "            values='Value',\n",
        "            aggfunc='first'\n",
        "        ).reset_index()\n",
        "\n",
        "        # Save both formats\n",
        "        df.to_csv(f\"{CONFIG['output_dir']}/WHO_GHO_Malaria_Raw.csv\", index=False)\n",
        "        pivot_df.to_csv(f\"{CONFIG['output_dir']}/WHO_GHO_Malaria_Pivot.csv\", index=False)\n",
        "\n",
        "        print(f\"\\n   âœ… Total records collected: {len(df)}\")\n",
        "        print(f\"   âœ… Unique country-years: {len(pivot_df)}\")\n",
        "        print(f\"   ðŸ“ Saved to: {CONFIG['output_dir']}/\")\n",
        "\n",
        "        return pivot_df\n",
        "    else:\n",
        "        print(\"\\n   âŒ No WHO GHO data collected\")\n",
        "        return None\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3.3 NASA POWER - Climate Data\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def download_nasa_climate_data():\n",
        "    \"\"\"\n",
        "    Download climate data from NASA POWER API for all major cities\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Daily climate data for all cities\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nðŸ“¥ Downloading NASA POWER climate data...\")\n",
        "\n",
        "    base_url = \"https://power.larc.nasa.gov/api/temporal/daily/point\"\n",
        "\n",
        "    start_date = f\"{CONFIG['start_year']}0101\"\n",
        "    end_date = f\"{CONFIG['end_year']}1231\"\n",
        "\n",
        "    all_climate_data = []\n",
        "\n",
        "    for city, coords in CITIES.items():\n",
        "        print(f\"\\n   Processing: {city}, {coords['country']}\")\n",
        "\n",
        "        params = {\n",
        "            'latitude': coords['lat'],\n",
        "            'longitude': coords['lon'],\n",
        "            'start': start_date,\n",
        "            'end': end_date,\n",
        "            'community': 'AG',\n",
        "            'parameters': 'T2M,T2M_MAX,T2M_MIN,PRECTOTCORR,RH2M',\n",
        "            'format': 'JSON',\n",
        "            'user': 'anonymous'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(base_url, params=params, timeout=120)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            data = response.json()\n",
        "\n",
        "            if 'properties' in data and 'parameter' in data['properties']:\n",
        "                params_data = data['properties']['parameter']\n",
        "                dates = list(params_data['T2M'].keys())\n",
        "\n",
        "                for date in dates:\n",
        "                    all_climate_data.append({\n",
        "                        'City': city,\n",
        "                        'Country': coords['country'],\n",
        "                        'Latitude': coords['lat'],\n",
        "                        'Longitude': coords['lon'],\n",
        "                        'Date': pd.to_datetime(date, format='%Y%m%d'),\n",
        "                        'Temperature_Avg_C': params_data['T2M'].get(date),\n",
        "                        'Temperature_Max_C': params_data['T2M_MAX'].get(date),\n",
        "                        'Temperature_Min_C': params_data['T2M_MIN'].get(date),\n",
        "                        'Precipitation_mm': params_data['PRECTOTCORR'].get(date),\n",
        "                        'Humidity_Percent': params_data['RH2M'].get(date)\n",
        "                    })\n",
        "\n",
        "                print(f\"      âœ… Downloaded {len(dates)} days of data\")\n",
        "            else:\n",
        "                print(f\"      âŒ Invalid response structure\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"      âŒ Error: {e}\")\n",
        "\n",
        "        time.sleep(2)  # Rate limiting\n",
        "\n",
        "    if all_climate_data:\n",
        "        df = pd.DataFrame(all_climate_data)\n",
        "\n",
        "        # Replace -999 (missing values) with NaN\n",
        "        numeric_columns = ['Temperature_Avg_C', 'Temperature_Max_C', 'Temperature_Min_C',\n",
        "                          'Precipitation_mm', 'Humidity_Percent']\n",
        "        for col in numeric_columns:\n",
        "            df[col] = df[col].replace(-999, pd.NA)\n",
        "\n",
        "        # Save daily data\n",
        "        daily_file = f\"{CONFIG['output_dir']}/NASA_Climate_Daily.csv\"\n",
        "        df.to_csv(daily_file, index=False)\n",
        "\n",
        "        print(f\"\\n   âœ… Total climate records: {len(df)}\")\n",
        "        print(f\"   ðŸ“ Saved to: {daily_file}\")\n",
        "\n",
        "        return df\n",
        "    else:\n",
        "        print(\"\\n   âŒ No climate data collected\")\n",
        "        return None\n",
        "\n",
        "def aggregate_climate_to_monthly(daily_data):\n",
        "    \"\"\"\n",
        "    Aggregate daily climate data to monthly averages\n",
        "\n",
        "    Args:\n",
        "        daily_data (DataFrame): Daily climate data\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Monthly aggregated climate data\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nðŸ“Š Aggregating climate data to monthly...\")\n",
        "\n",
        "    df = daily_data.copy()\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "\n",
        "    # Aggregate by country, year, and month\n",
        "    monthly = df.groupby(['Country', 'Year', 'Month']).agg({\n",
        "        'Temperature_Avg_C': 'mean',\n",
        "        'Temperature_Max_C': 'max',\n",
        "        'Temperature_Min_C': 'min',\n",
        "        'Precipitation_mm': 'sum',  # Total precipitation\n",
        "        'Humidity_Percent': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Create date column\n",
        "    monthly['Date'] = pd.to_datetime(monthly[['Year', 'Month']].assign(Day=1))\n",
        "\n",
        "    # Save\n",
        "    monthly_file = f\"{CONFIG['output_dir']}/NASA_Climate_Monthly.csv\"\n",
        "    monthly.to_csv(monthly_file, index=False)\n",
        "\n",
        "    print(f\"   âœ… Monthly records: {len(monthly)}\")\n",
        "    print(f\"   ðŸ“ Saved to: {monthly_file}\")\n",
        "\n",
        "    return monthly\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3.4 World Bank - Socioeconomic Indicators\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def download_worldbank_data():\n",
        "    \"\"\"\n",
        "    Download socioeconomic indicators from World Bank API\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: World Bank indicators for target countries\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nðŸ“¥ Downloading World Bank socioeconomic indicators...\")\n",
        "\n",
        "    base_url = \"https://api.worldbank.org/v2/country\"\n",
        "\n",
        "    # Key indicators\n",
        "    indicators = {\n",
        "        'SP.POP.TOTL': 'Total_Population',\n",
        "        'SP.RUR.TOTL.ZS': 'Rural_Population_Percent',\n",
        "        'SP.URB.TOTL.IN.ZS': 'Urban_Population_Percent',\n",
        "        'NY.GDP.PCAP.CD': 'GDP_per_Capita_USD',\n",
        "        'SH.H2O.BASW.ZS': 'Basic_Water_Access_Percent',\n",
        "        'SH.STA.BASS.ZS': 'Basic_Sanitation_Access_Percent',\n",
        "        'SH.DYN.MORT': 'Under5_Mortality_per_1000',\n",
        "        'SH.MED.BEDS.ZS': 'Hospital_Beds_per_1000'\n",
        "    }\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    for country_code in CONFIG['countries_iso']:\n",
        "        for indicator_code, indicator_name in indicators.items():\n",
        "            print(f\"   Processing: {indicator_name} for {CONFIG['country_names'][country_code]}\")\n",
        "\n",
        "            url = f\"{base_url}/{country_code}/indicator/{indicator_code}\"\n",
        "            params = {\n",
        "                'date': f\"{CONFIG['start_year']}:{CONFIG['end_year']}\",\n",
        "                'format': 'json',\n",
        "                'per_page': 500\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                response = requests.get(url, params=params, timeout=30)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "\n",
        "                    if len(data) > 1 and data[1]:\n",
        "                        for record in data[1]:\n",
        "                            if record['value'] is not None:\n",
        "                                all_data.append({\n",
        "                                    'Country': CONFIG['country_names'][country_code],\n",
        "                                    'Country_Code': country_code,\n",
        "                                    'Year': int(record['date']),\n",
        "                                    'Indicator': indicator_name,\n",
        "                                    'Value': record['value']\n",
        "                                })\n",
        "                        print(f\"      âœ… Retrieved {len(data[1])} records\")\n",
        "                    else:\n",
        "                        print(f\"      âš ï¸  No data available\")\n",
        "                else:\n",
        "                    print(f\"      âŒ HTTP {response.status_code}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"      âŒ Error: {e}\")\n",
        "\n",
        "            time.sleep(0.3)  # Rate limiting\n",
        "\n",
        "    if all_data:\n",
        "        df = pd.DataFrame(all_data)\n",
        "\n",
        "        # Pivot to wide format\n",
        "        pivot_df = df.pivot_table(\n",
        "            index=['Country', 'Country_Code', 'Year'],\n",
        "            columns='Indicator',\n",
        "            values='Value',\n",
        "            aggfunc='first'\n",
        "        ).reset_index()\n",
        "\n",
        "        # Save\n",
        "        pivot_file = f\"{CONFIG['output_dir']}/WorldBank_Indicators.csv\"\n",
        "        pivot_df.to_csv(pivot_file, index=False)\n",
        "\n",
        "        print(f\"\\n   âœ… Total records: {len(df)}\")\n",
        "        print(f\"   âœ… Country-years: {len(pivot_df)}\")\n",
        "        print(f\"   ðŸ“ Saved to: {pivot_file}\")\n",
        "\n",
        "        return pivot_df\n",
        "    else:\n",
        "        print(\"\\n   âŒ No World Bank data collected\")\n",
        "        return None\n",
        "\n",
        "print(\"âœ… All data collection functions defined!\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 4: DATA MERGING & INTEGRATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"SECTION 4: Defining Data Merging Functions\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def create_master_dataset():\n",
        "    \"\"\"\n",
        "    Merge all collected data sources into a single master dataset\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Integrated master dataset\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nðŸ”— Creating master dataset by merging all sources...\")\n",
        "\n",
        "    # Initialize master dataframe\n",
        "    master_df = None\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # Load OWID data (primary source for disease data)\n",
        "    # ---------------------------------------------------------------------\n",
        "    try:\n",
        "        owid_deaths = pd.read_csv(f\"{CONFIG['output_dir']}/OWID_Malaria_Deaths.csv\")\n",
        "        owid_incidence = pd.read_csv(f\"{CONFIG['output_dir']}/OWID_Malaria_Incidence.csv\")\n",
        "\n",
        "        # Start with deaths data\n",
        "        master_df = owid_deaths[['Country', 'Year']].drop_duplicates()\n",
        "\n",
        "        # Merge death columns\n",
        "        death_cols = [col for col in owid_deaths.columns if col not in ['Country', 'Year', 'Entity', 'Code']]\n",
        "        for col in death_cols:\n",
        "            master_df = master_df.merge(\n",
        "                owid_deaths[['Country', 'Year', col]],\n",
        "                on=['Country', 'Year'],\n",
        "                how='left',\n",
        "                suffixes=('', '_OWID_Deaths')\n",
        "            )\n",
        "\n",
        "        # Merge incidence columns\n",
        "        inc_cols = [col for col in owid_incidence.columns if col not in ['Country', 'Year', 'Entity', 'Code']]\n",
        "        for col in inc_cols:\n",
        "            master_df = master_df.merge(\n",
        "                owid_incidence[['Country', 'Year', col]],\n",
        "                on=['Country', 'Year'],\n",
        "                how='left',\n",
        "                suffixes=('', '_OWID_Inc')\n",
        "            )\n",
        "\n",
        "        print(\"   âœ… Merged OWID malaria data\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"   âš ï¸  OWID data not found, skipping...\")\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # Merge WHO GHO data\n",
        "    # ---------------------------------------------------------------------\n",
        "    try:\n",
        "        who_gho = pd.read_csv(f\"{CONFIG['output_dir']}/WHO_GHO_Malaria_Pivot.csv\")\n",
        "\n",
        "        if master_df is None:\n",
        "            master_df = who_gho[['Country', 'Year']].drop_duplicates()\n",
        "\n",
        "        who_cols = [col for col in who_gho.columns if col not in ['Country', 'Year', 'Country_Code']]\n",
        "\n",
        "        master_df = master_df.merge(\n",
        "            who_gho,\n",
        "            on=['Country', 'Year'],\n",
        "            how='outer',\n",
        "            suffixes=('', '_WHO')\n",
        "        )\n",
        "\n",
        "        print(\"   âœ… Merged WHO GHO data\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"   âš ï¸  WHO GHO data not found, skipping...\")\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # Merge Climate data\n",
        "    # ---------------------------------------------------------------------\n",
        "    try:\n",
        "        climate = pd.read_csv(f\"{CONFIG['output_dir']}/NASA_Climate_Monthly.csv\")\n",
        "\n",
        "        # Aggregate by country and year\n",
        "        climate_yearly = climate.groupby(['Country', 'Year']).agg({\n",
        "            'Temperature_Avg_C': 'mean',\n",
        "            'Temperature_Max_C': 'max',\n",
        "            'Temperature_Min_C': 'min',\n",
        "            'Precipitation_mm': 'sum',\n",
        "            'Humidity_Percent': 'mean'\n",
        "        }).reset_index()\n",
        "\n",
        "        master_df = master_df.merge(\n",
        "            climate_yearly,\n",
        "            on=['Country', 'Year'],\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        print(\"   âœ… Merged NASA climate data\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"   âš ï¸  Climate data not found, skipping...\")\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # Merge World Bank data\n",
        "    # ---------------------------------------------------------------------\n",
        "    try:\n",
        "        worldbank = pd.read_csv(f\"{CONFIG['output_dir']}/WorldBank_Indicators.csv\")\n",
        "\n",
        "        wb_cols = [col for col in worldbank.columns if col not in ['Country', 'Year', 'Country_Code']]\n",
        "\n",
        "        master_df = master_df.merge(\n",
        "            worldbank[['Country', 'Year'] + wb_cols],\n",
        "            on=['Country', 'Year'],\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        print(\"   âœ… Merged World Bank indicators\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"   âš ï¸  World Bank data not found, skipping...\")\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # Feature Engineering\n",
        "    # ---------------------------------------------------------------------\n",
        "    if master_df is not None:\n",
        "        print(\"\\nðŸ”§ Performing feature engineering...\")\n",
        "\n",
        "        # Sort by country and year\n",
        "        master_df = master_df.sort_values(['Country', 'Year']).reset_index(drop=True)\n",
        "\n",
        "        # Create temporal features\n",
        "        master_df['Year_Since_Start'] = master_df['Year'] - CONFIG['start_year']\n",
        "        master_df['Year_Normalized'] = (master_df['Year'] - CONFIG['start_year']) / (CONFIG['end_year'] - CONFIG['start_year'])\n",
        "\n",
        "        # Create year-over-year change features for key indicators\n",
        "        numeric_cols = master_df.select_dtypes(include=[np.number]).columns\n",
        "        numeric_cols = [col for col in numeric_cols if col not in ['Year', 'Year_Since_Start', 'Year_Normalized']]\n",
        "\n",
        "        for col in numeric_cols:\n",
        "            master_df[f'{col}_YoY_Change'] = master_df.groupby('Country')[col].pct_change()\n",
        "            master_df[f'{col}_YoY_Diff'] = master_df.groupby('Country')[col].diff()\n",
        "\n",
        "        # Remove columns with > 80% missing values\n",
        "        threshold = 0.8\n",
        "        missing_pct = master_df.isnull().sum() / len(master_df)\n",
        "        cols_to_keep = missing_pct[missing_pct < threshold].index.tolist()\n",
        "\n",
        "        dropped_cols = [col for col in master_df.columns if col not in cols_to_keep]\n",
        "        if dropped_cols:\n",
        "            print(f\"   âš ï¸  Dropped {len(dropped_cols)} columns with >80% missing values\")\n",
        "\n",
        "        master_df = master_df[cols_to_keep]\n",
        "\n",
        "        # Save master dataset\n",
        "        master_file = f\"{CONFIG['output_dir']}/MASTER_Dataset.csv\"\n",
        "        master_df.to_csv(master_file, index=False)\n",
        "\n",
        "        print(f\"\\n   âœ… Master dataset created!\")\n",
        "        print(f\"   ðŸ“Š Shape: {master_df.shape}\")\n",
        "        print(f\"   ðŸ“Š Countries: {master_df['Country'].unique().tolist()}\")\n",
        "        print(f\"   ðŸ“Š Years: {master_df['Year'].min()} - {master_df['Year'].max()}\")\n",
        "        print(f\"   ðŸ“ Saved to: {master_file}\")\n",
        "\n",
        "        return master_df\n",
        "    else:\n",
        "        print(\"\\n   âŒ Could not create master dataset - no data sources available\")\n",
        "        return None\n",
        "\n",
        "print(\"âœ… Data merging functions defined!\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 5: DATA QUALITY & VISUALIZATION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"SECTION 5: Defining Quality Check & Visualization Functions\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def generate_data_quality_report(master_df):\n",
        "    \"\"\"\n",
        "    Generate comprehensive data quality report\n",
        "\n",
        "    Args:\n",
        "        master_df (DataFrame): Master dataset\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nðŸ“Š Generating data quality report...\")\n",
        "\n",
        "    # Basic statistics\n",
        "    report = {\n",
        "        'Total Records': len(master_df),\n",
        "        'Countries': master_df['Country'].nunique(),\n",
        "        'Years': master_df['Year'].nunique(),\n",
        "        'Total Features': len(master_df.columns),\n",
        "        'Numeric Features': len(master_df.select_dtypes(include=[np.number]).columns),\n",
        "        'Object Features': len(master_df.select_dtypes(include=['object']).columns)\n",
        "    }\n",
        "\n",
        "    print(\"\\n   ðŸ“‹ Dataset Overview:\")\n",
        "    for key, value in report.items():\n",
        "        print(f\"      {key}: {value}\")\n",
        "\n",
        "    # Missing data analysis\n",
        "    missing_data = pd.DataFrame({\n",
        "        'Column': master_df.columns,\n",
        "        'Missing_Count': master_df.isnull().sum(),\n",
        "        'Missing_Percent': (master_df.isnull().sum() / len(master_df) * 100).round(2)\n",
        "    }).sort_values('Missing_Percent', ascending=False)\n",
        "\n",
        "    missing_file = f\"{CONFIG['output_dir']}/Data_Quality_Report.csv\"\n",
        "    missing_data.to_csv(missing_file, index=False)\n",
        "\n",
        "    print(f\"\\n   ðŸ“ Quality report saved to: {missing_file}\")\n",
        "\n",
        "    # Print top 10 columns with missing data\n",
        "    if missing_data['Missing_Percent'].max() > 0:\n",
        "        print(\"\\n   âš ï¸  Top 10 columns with missing data:\")\n",
        "        top_missing = missing_data[missing_data['Missing_Percent'] > 0].head(10)\n",
        "        for _, row in top_missing.iterrows():\n",
        "            print(f\"      {row['Column']}: {row['Missing_Percent']}%\")\n",
        "    else:\n",
        "        print(\"\\n   âœ… No missing data detected!\")\n",
        "\n",
        "    return missing_data\n",
        "\n",
        "def create_eda_visualizations(master_df):\n",
        "    \"\"\"\n",
        "    Create exploratory data analysis visualizations\n",
        "\n",
        "    Args:\n",
        "        master_df (DataFrame): Master dataset\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nðŸ“Š Creating exploratory data analysis visualizations...\")\n",
        "\n",
        "    # Set style\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    plt.rcParams['figure.figsize'] = (16, 12)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Figure 1: Temporal Trends\n",
        "    # -------------------------------------------------------------------------\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "\n",
        "    # Find malaria-related columns\n",
        "    malaria_cols = [col for col in master_df.columns\n",
        "                   if any(x in col.lower() for x in ['malaria', 'death', 'case', 'incidence'])]\n",
        "\n",
        "    if malaria_cols:\n",
        "        # Plot 1: Malaria cases/deaths over time\n",
        "        for country in master_df['Country'].unique():\n",
        "            country_data = master_df[master_df['Country'] == country]\n",
        "            if malaria_cols[0] in country_data.columns:\n",
        "                axes[0, 0].plot(country_data['Year'], country_data[malaria_cols[0]],\n",
        "                              marker='o', label=country, linewidth=2)\n",
        "\n",
        "        axes[0, 0].set_title('Malaria Indicator Trends by Country', fontsize=14, fontweight='bold')\n",
        "        axes[0, 0].set_xlabel('Year')\n",
        "        axes[0, 0].set_ylabel(malaria_cols[0])\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Temperature trends\n",
        "    if 'Temperature_Avg_C' in master_df.columns:\n",
        "        for country in master_df['Country'].unique():\n",
        "            country_data = master_df[master_df['Country'] == country]\n",
        "            axes[0, 1].plot(country_data['Year'], country_data['Temperature_Avg_C'],\n",
        "                          marker='s', label=country, linewidth=2)\n",
        "\n",
        "        axes[0, 1].set_title('Average Temperature Trends', fontsize=14, fontweight='bold')\n",
        "        axes[0, 1].set_xlabel('Year')\n",
        "        axes[0, 1].set_ylabel('Temperature (Â°C)')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 3: Precipitation patterns\n",
        "    if 'Precipitation_mm' in master_df.columns:\n",
        "        for country in master_df['Country'].unique():\n",
        "            country_data = master_df[master_df['Country'] == country]\n",
        "            axes[1, 0].plot(country_data['Year'], country_data['Precipitation_mm'],\n",
        "                          marker='^', label=country, linewidth=2)\n",
        "\n",
        "        axes[1, 0].set_title('Annual Precipitation Patterns', fontsize=14, fontweight='bold')\n",
        "        axes[1, 0].set_xlabel('Year')\n",
        "        axes[1, 0].set_ylabel('Precipitation (mm)')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 4: Population growth\n",
        "    pop_cols = [col for col in master_df.columns if 'population' in col.lower()]\n",
        "    if pop_cols:\n",
        "        for country in master_df['Country'].unique():\n",
        "            country_data = master_df[master_df['Country'] == country]\n",
        "            if pop_cols[0] in country_data.columns:\n",
        "                axes[1, 1].plot(country_data['Year'], country_data[pop_cols[0]],\n",
        "                              marker='d', label=country, linewidth=2)\n",
        "\n",
        "        axes[1, 1].set_title('Population Growth Trends', fontsize=14, fontweight='bold')\n",
        "        axes[1, 1].set_xlabel('Year')\n",
        "        axes[1, 1].set_ylabel('Population')\n",
        "        axes[1, 1].legend()\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    trends_file = f\"{CONFIG['plots_dir']}/01_Temporal_Trends.png\"\n",
        "    plt.savefig(trends_file, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"   âœ… Saved: {trends_file}\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Figure 2: Correlation Matrix\n",
        "    # -------------------------------------------------------------------------\n",
        "    numeric_df = master_df.select_dtypes(include=[np.number])\n",
        "\n",
        "    # Remove YoY change columns for cleaner correlation matrix\n",
        "    numeric_df = numeric_df[[col for col in numeric_df.columns\n",
        "                            if not any(x in col for x in ['YoY_Change', 'YoY_Diff', 'Year_Since_Start'])]]\n",
        "\n",
        "    if len(numeric_df.columns) > 1:\n",
        "        plt.figure(figsize=(20, 16))\n",
        "        corr_matrix = numeric_df.corr()\n",
        "\n",
        "        sns.heatmap(corr_matrix,\n",
        "                   annot=False,\n",
        "                   cmap='coolwarm',\n",
        "                   center=0,\n",
        "                   linewidths=0.5,\n",
        "                   cbar_kws={'label': 'Correlation Coefficient'})\n",
        "\n",
        "        plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        corr_file = f\"{CONFIG['plots_dir']}/02_Correlation_Matrix.png\"\n",
        "        plt.savefig(corr_file, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"   âœ… Saved: {corr_file}\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Figure 3: Country Comparisons\n",
        "    # -------------------------------------------------------------------------\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    countries = master_df['Country'].unique()\n",
        "    colors = sns.color_palette(\"husl\", len(countries))\n",
        "\n",
        "    # Plot 1: Malaria distribution by country\n",
        "    if malaria_cols:\n",
        "        for idx, country in enumerate(countries):\n",
        "            country_data = master_df[master_df['Country'] == country][malaria_cols[0]].dropna()\n",
        "            if len(country_data) > 0:\n",
        "                axes[0, 0].hist(country_data, alpha=0.6, label=country,\n",
        "                              color=colors[idx], bins=12, edgecolor='black')\n",
        "\n",
        "        axes[0, 0].set_title('Distribution of Malaria Cases by Country',\n",
        "                            fontsize=12, fontweight='bold')\n",
        "        axes[0, 0].set_xlabel('Cases')\n",
        "        axes[0, 0].set_ylabel('Frequency')\n",
        "        axes[0, 0].legend()\n",
        "\n",
        "    # Plot 2: Temperature distribution by country\n",
        "    if 'Temperature_Avg_C' in master_df.columns:\n",
        "        temp_data = [master_df[master_df['Country'] == country]['Temperature_Avg_C'].dropna()\n",
        "                     for country in countries]\n",
        "\n",
        "        bp = axes[0, 1].boxplot(temp_data, labels=countries, patch_artist=True)\n",
        "\n",
        "        for patch, color in zip(bp['boxes'], colors):\n",
        "            patch.set_facecolor(color)\n",
        "\n",
        "        axes[0, 1].set_title('Temperature Distribution by Country',\n",
        "                            fontsize=12, fontweight='bold')\n",
        "        axes[0, 1].set_xlabel('Country')\n",
        "        axes[0, 1].set_ylabel('Temperature (Â°C)')\n",
        "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Plot 3: Precipitation distribution by country\n",
        "    if 'Precipitation_mm' in master_df.columns:\n",
        "        precip_data = [master_df[master_df['Country'] == country]['Precipitation_mm'].dropna()\n",
        "                      for country in countries]\n",
        "\n",
        "        bp = axes[1, 0].boxplot(precip_data, labels=countries, patch_artist=True)\n",
        "\n",
        "        for patch, color in zip(bp['boxes'], colors):\n",
        "            patch.set_facecolor(color)\n",
        "\n",
        "        axes[1, 0].set_title('Precipitation Distribution by Country',\n",
        "                            fontsize=12, fontweight='bold')\n",
        "        axes[1, 0].set_xlabel('Country')\n",
        "        axes[1, 0].set_ylabel('Precipitation (mm)')\n",
        "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Plot 4: Average malaria by country (bar chart)\n",
        "    if malaria_cols:\n",
        "        country_means = master_df.groupby('Country')[malaria_cols[0]].mean().sort_values()\n",
        "\n",
        "        bars = axes[1, 1].barh(range(len(country_means)), country_means.values, color=colors)\n",
        "        axes[1, 1].set_yticks(range(len(country_means)))\n",
        "        axes[1, 1].set_yticklabels(country_means.index)\n",
        "\n",
        "        axes[1, 1].set_title(f'Average {malaria_cols[0]} by Country',\n",
        "                            fontsize=12, fontweight='bold')\n",
        "        axes[1, 1].set_xlabel('Average Value')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    comparison_file = f\"{CONFIG['plots_dir']}/03_Country_Comparisons.png\"\n",
        "    plt.savefig(comparison_file, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"   âœ… Saved: {comparison_file}\")\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Figure 4: Key Statistics Summary\n",
        "    # -------------------------------------------------------------------------\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    # Plot 1: Data completeness by country\n",
        "    completeness = []\n",
        "    for country in countries:\n",
        "        country_data = master_df[master_df['Country'] == country]\n",
        "        complete_pct = (1 - country_data.isnull().sum().sum() /\n",
        "                       (len(country_data) * len(country_data.columns))) * 100\n",
        "        completeness.append(complete_pct)\n",
        "\n",
        "    axes[0, 0].bar(countries, completeness, color=colors, edgecolor='black')\n",
        "    axes[0, 0].set_title('Data Completeness by Country', fontsize=12, fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Country')\n",
        "    axes[0, 0].set_ylabel('Completeness (%)')\n",
        "    axes[0, 0].set_ylim([0, 100])\n",
        "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for i, v in enumerate(completeness):\n",
        "        axes[0, 0].text(i, v + 2, f'{v:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # Plot 2: Number of records per country\n",
        "    records_per_country = master_df['Country'].value_counts().sort_index()\n",
        "\n",
        "    axes[0, 1].bar(records_per_country.index, records_per_country.values,\n",
        "                   color=colors, edgecolor='black')\n",
        "    axes[0, 1].set_title('Number of Records by Country', fontsize=12, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Country')\n",
        "    axes[0, 1].set_ylabel('Number of Records')\n",
        "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    for i, v in enumerate(records_per_country.values):\n",
        "        axes[0, 1].text(i, v + 0.5, str(v), ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # Plot 3: Feature category distribution\n",
        "    feature_categories = {\n",
        "        'Disease': len([col for col in master_df.columns\n",
        "                       if any(x in col.lower() for x in ['malaria', 'death', 'case', 'disease'])]),\n",
        "        'Climate': len([col for col in master_df.columns\n",
        "                       if any(x in col.lower() for x in ['temperature', 'precipitation', 'humidity'])]),\n",
        "        'Socioeconomic': len([col for col in master_df.columns\n",
        "                             if any(x in col.lower() for x in ['population', 'gdp', 'water', 'sanitation'])]),\n",
        "        'Temporal': len([col for col in master_df.columns\n",
        "                        if any(x in col.lower() for x in ['year', 'date'])]),\n",
        "        'Other': len(master_df.columns)\n",
        "    }\n",
        "\n",
        "    # Adjust 'Other' to not double count\n",
        "    feature_categories['Other'] = feature_categories['Other'] - sum([v for k, v in feature_categories.items() if k != 'Other'])\n",
        "\n",
        "    axes[1, 0].pie(feature_categories.values(),\n",
        "                   labels=feature_categories.keys(),\n",
        "                   autopct='%1.1f%%',\n",
        "                   startangle=90,\n",
        "                   colors=sns.color_palette(\"pastel\"))\n",
        "    axes[1, 0].set_title('Feature Distribution by Category', fontsize=12, fontweight='bold')\n",
        "\n",
        "    # Plot 4: Missing data summary\n",
        "    missing_by_feature_type = {\n",
        "        'Disease': master_df[[col for col in master_df.columns\n",
        "                             if any(x in col.lower() for x in ['malaria', 'death', 'case'])]].isnull().sum().sum(),\n",
        "        'Climate': master_df[[col for col in master_df.columns\n",
        "                             if any(x in col.lower() for x in ['temperature', 'precipitation', 'humidity'])]].isnull().sum().sum(),\n",
        "        'Socioeconomic': master_df[[col for col in master_df.columns\n",
        "                                   if any(x in col.lower() for x in ['population', 'gdp', 'water', 'sanitation'])]].isnull().sum().sum()\n",
        "    }\n",
        "\n",
        "    axes[1, 1].bar(missing_by_feature_type.keys(), missing_by_feature_type.values(),\n",
        "                  color=sns.color_palette(\"pastel\"), edgecolor='black')\n",
        "    axes[1, 1].set_title('Missing Values by Feature Category', fontsize=12, fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Feature Category')\n",
        "    axes[1, 1].set_ylabel('Missing Values Count')\n",
        "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    for i, v in enumerate(missing_by_feature_type.values()):\n",
        "        axes[1, 1].text(i, v + max(missing_by_feature_type.values())*0.02,\n",
        "                      str(v), ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    summary_file = f\"{CONFIG['plots_dir']}/04_Summary_Statistics.png\"\n",
        "    plt.savefig(summary_file, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"   âœ… Saved: {summary_file}\")\n",
        "\n",
        "    print(\"\\n   âœ… All visualizations created successfully!\")\n",
        "\n",
        "def generate_descriptive_statistics(master_df):\n",
        "    \"\"\"\n",
        "    Generate and save descriptive statistics\n",
        "\n",
        "    Args:\n",
        "        master_df (DataFrame): Master dataset\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nðŸ“Š Generating descriptive statistics...\")\n",
        "\n",
        "    # Overall statistics\n",
        "    desc_stats = master_df.describe().T\n",
        "    desc_stats_file = f\"{CONFIG['output_dir']}/Descriptive_Statistics_Overall.csv\"\n",
        "    desc_stats.to_csv(desc_stats_file)\n",
        "    print(f\"   âœ… Saved overall statistics to: {desc_stats_file}\")\n",
        "\n",
        "    # Statistics by country\n",
        "    numeric_cols = master_df.select_dtypes(include=[np.number]).columns\n",
        "    numeric_cols = [col for col in numeric_cols if col not in ['Year', 'Year_Since_Start', 'Year_Normalized']]\n",
        "\n",
        "    country_stats_list = []\n",
        "\n",
        "    for country in master_df['Country'].unique():\n",
        "        country_data = master_df[master_df['Country'] == country][numeric_cols]\n",
        "        stats = country_data.describe().T\n",
        "        stats['Country'] = country\n",
        "        country_stats_list.append(stats)\n",
        "\n",
        "    country_stats = pd.concat(country_stats_list)\n",
        "    country_stats_file = f\"{CONFIG['output_dir']}/Descriptive_Statistics_By_Country.csv\"\n",
        "    country_stats.to_csv(country_stats_file)\n",
        "    print(f\"   âœ… Saved country statistics to: {country_stats_file}\")\n",
        "\n",
        "print(\"âœ… Quality check and visualization functions defined!\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 6: MASTER EXECUTION PIPELINE\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"SECTION 6: Master Execution Pipeline\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def execute_complete_pipeline():\n",
        "    \"\"\"\n",
        "    Execute the complete data collection and processing pipeline\n",
        "\n",
        "    Returns:\n",
        "        dict: Results summary\n",
        "    \"\"\"\n",
        "\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"ðŸš€ STARTING COMPLETE DATA COLLECTION PIPELINE\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "    results = {\n",
        "        'start_time': start_time,\n",
        "        'data_sources': {},\n",
        "        'files_created': []\n",
        "    }\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # STEP 1: Download OWID Malaria Data\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 1/6: Downloading Our World in Data (OWID) Malaria Data\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    try:\n",
        "        owid_data = download_owid_malaria_data()\n",
        "        results['data_sources']['owid'] = 'SUCCESS' if owid_data else 'FAILED'\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ OWID download failed: {e}\")\n",
        "        results['data_sources']['owid'] = 'FAILED'\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # STEP 2: Download WHO GHO Data\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 2/6: Downloading WHO Global Health Observatory (GHO) Data\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    try:\n",
        "        who_data = download_who_gho_data()\n",
        "        results['data_sources']['who_gho'] = 'SUCCESS' if who_data is not None else 'FAILED'\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ WHO GHO download failed: {e}\")\n",
        "        results['data_sources']['who_gho'] = 'FAILED'\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # STEP 3: Download NASA Climate Data\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 3/6: Downloading NASA POWER Climate Data\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    try:\n",
        "        climate_daily = download_nasa_climate_data()\n",
        "        if climate_daily is not None:\n",
        "            climate_monthly = aggregate_climate_to_monthly(climate_daily)\n",
        "            results['data_sources']['nasa_climate'] = 'SUCCESS'\n",
        "        else:\n",
        "            results['data_sources']['nasa_climate'] = 'FAILED'\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ NASA climate download failed: {e}\")\n",
        "        results['data_sources']['nasa_climate'] = 'FAILED'\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # STEP 4: Download World Bank Data\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 4/6: Downloading World Bank Socioeconomic Indicators\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    try:\n",
        "        wb_data = download_worldbank_data()\n",
        "        results['data_sources']['worldbank'] = 'SUCCESS' if wb_data is not None else 'FAILED'\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ World Bank download failed: {e}\")\n",
        "        results['data_sources']['worldbank'] = 'FAILED'\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # STEP 5: Create Master Dataset\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 5/6: Creating Master Integrated Dataset\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    try:\n",
        "        master_df = create_master_dataset()\n",
        "        if master_df is not None:\n",
        "            results['data_sources']['master_dataset'] = 'SUCCESS'\n",
        "            results['master_df'] = master_df\n",
        "        else:\n",
        "            results['data_sources']['master_dataset'] = 'FAILED'\n",
        "            print(\"âš ï¸  WARNING: Master dataset creation failed!\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Master dataset creation failed: {e}\")\n",
        "        results['data_sources']['master_dataset'] = 'FAILED'\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # STEP 6: Generate Analysis & Visualizations\n",
        "    # -------------------------------------------------------------------------\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"STEP 6/6: Generating Quality Reports & Visualizations\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    if 'master_df' in results and results['master_df'] is not None:\n",
        "        try:\n",
        "            # Quality report\n",
        "            quality_report = generate_data_quality_report(results['master_df'])\n",
        "\n",
        "            # Descriptive statistics\n",
        "            generate_descriptive_statistics(results['master_df'])\n",
        "\n",
        "            # Visualizations\n",
        "            create_eda_visualizations(results['master_df'])\n",
        "\n",
        "            results['data_sources']['analysis'] = 'SUCCESS'\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Analysis generation failed: {e}\")\n",
        "            results['data_sources']['analysis'] = 'FAILED'\n",
        "    else:\n",
        "        print(\"âš ï¸  Skipping analysis - no master dataset available\")\n",
        "        results['data_sources']['analysis'] = 'SKIPPED'\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # FINAL SUMMARY\n",
        "    # -------------------------------------------------------------------------\n",
        "    end_time = datetime.now()\n",
        "    duration = end_time - start_time\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"ðŸ“Š PIPELINE EXECUTION SUMMARY\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(f\"\\nâ±ï¸  Execution Time: {duration}\")\n",
        "    print(f\"   Started: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"   Ended: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    print(\"\\nðŸ“¦ Data Source Status:\")\n",
        "    for source, status in results['data_sources'].items():\n",
        "        icon = \"âœ…\" if status == \"SUCCESS\" else \"âŒ\" if status == \"FAILED\" else \"âš ï¸\"\n",
        "        print(f\"   {icon} {source.upper()}: {status}\")\n",
        "\n",
        "    # List files created\n",
        "    print(\"\\nðŸ“ Files Created:\")\n",
        "\n",
        "    if os.path.exists(CONFIG['output_dir']):\n",
        "        data_files = [f for f in os.listdir(CONFIG['output_dir']) if f.endswith('.csv')]\n",
        "        for file in sorted(data_files):\n",
        "            file_path = os.path.join(CONFIG['output_dir'], file)\n",
        "            file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
        "            print(f\"   âœ… {file} ({file_size:.2f} MB)\")\n",
        "            results['files_created'].append(file)\n",
        "\n",
        "    if os.path.exists(CONFIG['plots_dir']):\n",
        "        plot_files = [f for f in os.listdir(CONFIG['plots_dir']) if f.endswith('.png')]\n",
        "        for file in sorted(plot_files):\n",
        "            print(f\"   âœ… {file}\")\n",
        "            results['files_created'].append(file)\n",
        "\n",
        "    # Success rate\n",
        "    success_count = sum(1 for status in results['data_sources'].values() if status == 'SUCCESS')\n",
        "    total_count = len([s for s in results['data_sources'].values() if s != 'SKIPPED'])\n",
        "    success_rate = (success_count / total_count * 100) if total_count > 0 else 0\n",
        "\n",
        "    print(f\"\\nðŸ“ˆ Success Rate: {success_count}/{total_count} ({success_rate:.1f}%)\")\n",
        "\n",
        "    if 'master_df' in results and results['master_df'] is not None:\n",
        "        print(\"\\nâœ…âœ…âœ… PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "        print(f\"\\nðŸŽ¯ Master Dataset Ready for Machine Learning!\")\n",
        "        print(f\"   Location: {CONFIG['output_dir']}/MASTER_Dataset.csv\")\n",
        "        print(f\"   Shape: {results['master_df'].shape}\")\n",
        "        print(f\"   Features: {len(results['master_df'].columns)}\")\n",
        "        print(f\"   Records: {len(results['master_df'])}\")\n",
        "    else:\n",
        "        print(\"\\nâš ï¸  PIPELINE COMPLETED WITH WARNINGS\")\n",
        "        print(\"   Some data sources failed. Check logs above for details.\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "    results['end_time'] = end_time\n",
        "    results['duration'] = duration\n",
        "    results['success_rate'] = success_rate\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"âœ… Master pipeline function defined!\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 7: EXECUTION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"SECTION 7: Execute Pipeline\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nðŸš€ Ready to execute the complete data collection pipeline!\")\n",
        "print(\"\\nThis will:\")\n",
        "print(\"   1. Download malaria data from OWID\")\n",
        "print(\"   2. Download WHO GHO malaria indicators\")\n",
        "print(\"   3. Download NASA climate data for 10 cities\")\n",
        "print(\"   4. Download World Bank socioeconomic indicators\")\n",
        "print(\"   5. Merge all data sources into master dataset\")\n",
        "print(\"   6. Generate quality reports and visualizations\")\n",
        "print(\"\\nâ±ï¸  Estimated time: 15-30 minutes\")\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "# Execute the pipeline\n",
        "results = execute_complete_pipeline()\n",
        "\n",
        "# =============================================================================\n",
        "# SECTION 8: POST-EXECUTION ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SECTION 8: Post-Execution Data Preview\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if 'master_df' in results and results['master_df'] is not None:\n",
        "    master_df = results['master_df']\n",
        "\n",
        "    print(\"\\nðŸ“Š Master Dataset Preview:\")\n",
        "    print(\"\\nFirst 5 rows:\")\n",
        "    display(master_df.head())\n",
        "\n",
        "    print(\"\\nDataset Info:\")\n",
        "    print(f\"   Shape: {master_df.shape}\")\n",
        "    print(f\"   Memory Usage: {master_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "    print(\"\\nColumn Names:\")\n",
        "    for i, col in enumerate(master_df.columns, 1):\n",
        "        print(f\"   {i}. {col}\")\n",
        "\n",
        "    print(\"\\nData Types:\")\n",
        "    print(master_df.dtypes.value_counts())\n",
        "\n",
        "    print(\"\\nMissing Data Summary:\")\n",
        "    missing_summary = master_df.isnull().sum()\n",
        "    missing_pct = (missing_summary / len(master_df) * 100).round(2)\n",
        "    missing_df = pd.DataFrame({\n",
        "        'Missing_Count': missing_summary,\n",
        "        'Missing_Percent': missing_pct\n",
        "    }).sort_values('Missing_Percent', ascending=False).head(10)\n",
        "    display(missing_df)\n",
        "\n",
        "    print(\"\\nâœ… Data collection complete! Ready for machine learning modeling.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nâš ï¸  Master dataset not available. Please check error messages above.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ðŸŽ‰ ALL DONE! Check the output folders for your data and visualizations.\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After execution completes, run this cell to download all data\n",
        "\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Create zip file with all results\n",
        "shutil.make_archive('/content/Central_Africa_Disease_Data', 'zip', '/content/data')\n",
        "shutil.make_archive('/content/Central_Africa_Plots', 'zip', '/content/plots')\n",
        "\n",
        "# Download\n",
        "files.download('/content/Central_Africa_Disease_Data.zip')\n",
        "files.download('/content/Central_Africa_Plots.zip')\n",
        "\n",
        "print(\"âœ… Downloads started! Check your browser's download folder.\")\n",
        "\n",
        "print(\"\\nâœ… Data collection complete! Ready for machine learning modeling.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\" ALL DONE! Check the output folders for your data and visualizations.\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "NzSNgMBGMOKe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}